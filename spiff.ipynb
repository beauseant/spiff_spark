{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "startt = timeit.default_timer()\n",
    "\n",
    "path = '/export/usuarios01/sblanco/Datos/repogit/spiff_spark/'\n",
    "path_out = '/export/usuarios01/sblanco/Datos/repogit/spiff_spark/Output/salida.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countWords ( cad ):\n",
    "    return ( [word for word in re.findall(r'\\b[^\\W\\d_]+\\b',cad)])\n",
    "\n",
    "\n",
    "def getData ( cad ):\n",
    "    \n",
    "        autor = '-1'\n",
    "        match_menciones = []\n",
    "        match_etiquetas = []\n",
    "        match_urls = []\n",
    "        \n",
    "        salida = []\n",
    "        try:\n",
    "            cad = cad.strip ()\n",
    "            \n",
    "            #El autor es la primera arroba:    \n",
    "            match = re.search (r'@[\\w\\.-]+', cad) \n",
    "            autor =  match.group(0)\n",
    "            \n",
    "            \n",
    "            #Dejamos la cadena sin el autor para seguir buscando en ella:\n",
    "            inicio_autor = cad.find('@') + 1\n",
    "            fin_autor = cad.find(' ',inicio_autor)\n",
    "            cad = cad[fin_autor+1:]\n",
    "\n",
    "            #Buscamos todas las menciones que aparezcan:\n",
    "            match_menciones = re.findall(r'@[\\w\\.-]+', cad)\n",
    "            \n",
    "            #y las etiquetas:\n",
    "            match_etiquetas = re.findall(r'#[\\w\\.-]+', cad)\n",
    "\n",
    "            #y las urls:\n",
    "            match_urls = re.findall(r'(https?://[^\\s]+)',cad)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "                match_menciones = [str(e)]\n",
    "                match_urls = [cad]\n",
    "        \n",
    "        \n",
    "        salida = [ match_menciones, match_etiquetas, match_urls ]\n",
    "            \n",
    "    \n",
    "        #salida[0]: autor\n",
    "        #salida[1][0]: lista de menciones, salida[1][0][0] primera mencion\n",
    "        #salida[1][1]: etiquetas\n",
    "        return autor, salida\n",
    "    \n",
    "def toCSV (rd_tweets, f):\n",
    "    \n",
    "    datasep = ';'\n",
    "    listsep = ','\n",
    "    \n",
    "    myF = open (f, 'w')\n",
    "    \n",
    "    for tw in rd_tweets.collect():\n",
    "        line = tw[0] + datasep\n",
    "        \n",
    "        for menc in tw[1][0]:\n",
    "            line = line + menc + listsep\n",
    "            \n",
    "        line = line + datasep\n",
    "        \n",
    "        for etiqu in tw[1][1]:\n",
    "            line = line + etiqu + listsep\n",
    "        \n",
    "        myF.write (line + '\\n')\n",
    "        \n",
    "    myF.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets fallidos: 7879 de 98756\n",
      "Los 10 autores con mas entradas son:[(u'@TunisiaTrends', 1815), (u'@SBZ_news', 1638), (u'@nawaat', 1524), (u'@Dima_Khatib', 1482), (u'@weddady', 1466), (u'@LiveWordCanada', 1234), (u'@halmustafa', 1149), (u'@TunObs', 1114), (u'@WikiActions', 958), (u'@PrimozVallant', 954)]\n"
     ]
    }
   ],
   "source": [
    "#tweets = sc.textFile('file:///export/usuarios01/sblanco/Datos/repogit/spiff_spark/Data/03_10_2016/red.txt')\n",
    "file_path = 'file://'+ path + '/Data/03_10_2016/todo.txt'\n",
    "tweets = sc.textFile(file_path)\n",
    "\n",
    "alldata = tweets.map ( getData )\n",
    "\n",
    "#Errores procesados, o buscar un autor concreto:\n",
    "author = '-1'\n",
    "print 'tweets fallidos: %s de %s' % (alldata.filter (lambda x : x[0]==author).count(), alldata.count())\n",
    "\n",
    "tweetsValidos = alldata.filter (lambda x : x[0] <> '-1')\n",
    "\n",
    "#Autor con mas entradas:\n",
    "count = 10\n",
    "moreentr = tweetsValidos.map ( lambda x: [x[0],1]).reduceByKey ( add ).takeOrdered( count ,key=lambda x: -x[1])\n",
    "print 'Los %s autores con mas entradas son:%s' % (str(count), str(moreentr))\n",
    "\n",
    "toCSV ( tweetsValidos, path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 10 etiquetas mas utilizadas son: [(u'#sidibouzid', 56296), (u'#SidiBouzid', 19533), (u'#Tunisia', 10806), (u'#Sidibouzid', 8323), (u'#tunisia', 5244), (u'#Tunisie', 3292), (u'#tunisie', 3131), (u'#OpTunisia', 2561), (u'#jasminrevolt', 2347), (u'#optunisia', 1523)]\n",
      "Etiquetas totales 4346\n"
     ]
    }
   ],
   "source": [
    "#expandimos la lista de etiquetas y las agrupamos\n",
    "listetique = tweetsValidos.flatMap ( lambda x : x[1][1]).map (lambda x : (x,1)).reduceByKey(add)\n",
    "topTenEtiq = listetique.takeOrdered( count ,key=lambda x: -x[1])\n",
    "\n",
    "print 'Las %s etiquetas mas utilizadas son: %s' % (str(count), str(topTenEtiq))\n",
    "print 'Etiquetas totales %s' % ( listetique.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 10 urls mas utilizadas son: [(u'http://wp.me/sEAit-blogger', 97), (u'http://bbc.in/ht3DvL', 90), (u'http://www.diigo.com/user/elhamalawy/Algeria', 80), (u'http://www.diigo.com/us...', 78), (u'http://tinyurl.com/5u4fzea', 53), (u'http://tinyurl.com/62s2gtd', 48), (u'http://curated.by/b/1S', 46), (u'http://tinyurl.com/6d9j88y', 46), (u'http://bit.ly/gouE0W', 43), (u'http://j.mp/gr6nTQ', 41)]\n",
      "Urls totales 18328\n"
     ]
    }
   ],
   "source": [
    "#expandimos la lista de urls y las agrupamos\n",
    "listUrl = tweetsValidos.flatMap ( lambda x : x[1][2]).map (lambda x : (x,1)).reduceByKey(add)\n",
    "topTenUrl = listUrl.takeOrdered( count ,key=lambda x: -x[1])\n",
    "print 'Las %s urls mas utilizadas son: %s' % (str(count), str(topTenUrl))\n",
    "print 'Urls totales %s' % ( listUrl.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:0.120576135317\n"
     ]
    }
   ],
   "source": [
    "print 'Total time:%s' % str ((timeit.default_timer() - startt)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
